{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTSf4ro-qU0d"
   },
   "source": [
    "## Бонусное задание: word2vec на PyTorch\n",
    "\n",
    "Как вы уже могли заметить, идея, лежащая в основе [word2vec](https://arxiv.org/pdf/1310.4546), достаточно общая. В данном задании вы реализуете его самостоятельно.\n",
    "\n",
    "Дисклеймер: не стоит удивляться тому, что реализация от `gensim` (или аналоги) обучается быстрее и работает точнее. Она использует множество доработок и ускорений, а также достаточно эффективный код. Ваша задача добиться промежуточных результатов за разумное время.\n",
    "\n",
    "P.s. Как ни странно, GPU в этом задании нам не потребуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements:__ if you're running locally, in the selected environment run the following command:\n",
    "\n",
    "```pip install --upgrade nltk bokeh umap-learn```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Oipe2DU8qfm7"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade nltk bokeh umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pqPU4Lu2qU0h"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import umap\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Fbx57z3qU0i",
    "outputId": "699ab6a7-41b9-402f-b842-039142bea06a"
   },
   "outputs": [],
   "source": [
    "# download the data:\n",
    "# !wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt -nc\n",
    "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "8EYy5dQpqU0i",
    "outputId": "57a393d2-d5f5-4fa9-f46d-1b268d7798ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feBEGI2eqU0j"
   },
   "source": [
    "Токенизация – первый шаг.\n",
    "Тексты, с которыми мы работаем, включают в себя пунктуацию, смайлики и прочие нестандартные токены, так что простой `str.split` не подойдет.\n",
    "\n",
    "Обратимся к `nltk` - библиотеку, нашла широкое применеие в области NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mWTvm50qU0j",
    "outputId": "18272985-9762-4288-8457-76fa78a44d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HzC05EHWqU0j"
   },
   "outputs": [],
   "source": [
    "data_tok = [\n",
    "    tokenizer.tokenize(\n",
    "        line.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
    "    )\n",
    "    for line in data\n",
    "]\n",
    "data_tok = [x for x in data_tok if len(x) >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6azRAgHIqU0k"
   },
   "source": [
    "Несколько проверок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Cp8UpxdWqU0k"
   },
   "outputs": [],
   "source": [
    "assert all(\n",
    "    isinstance(row, (list, tuple)) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "assert all(\n",
    "    all(isinstance(tok, str) for tok in row) for row in data_tok\n",
    "), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all(\"a\" <= x.lower() <= \"z\" for x in tok)\n",
    "assert all(\n",
    "    map(lambda l: not is_latin(l) or l.islower(), map(\" \".join, data_tok))\n",
    "), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37e7-MIhqU0x"
   },
   "source": [
    "Ниже заданы константы ширины окна контекста и проведена предобработка для построения skip-gram модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fugc1IomqU0x"
   },
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "window_radius = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_with_counter = Counter(chain.from_iterable(data_tok))\n",
    "\n",
    "word_count_dict = dict()\n",
    "for word, counter in vocabulary_with_counter.items():\n",
    "    if counter >= min_count:\n",
    "        word_count_dict[word] = counter\n",
    "\n",
    "vocabulary = set(word_count_dict.keys())\n",
    "del vocabulary_with_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Y8rlBfqqU0x"
   },
   "source": [
    "Пары `(слово, контекст)` на основе доступного датасета сгенерированы ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h2-aqyyaqU0x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 40220313 pairs of target and context words.\n"
     ]
    }
   ],
   "source": [
    "context_pairs = []\n",
    "\n",
    "for text in data_tok:\n",
    "    for i, central_word in enumerate(text):\n",
    "        context_indices = range(\n",
    "            max(0, i - window_radius), min(i + window_radius, len(text))\n",
    "        )\n",
    "        for j in context_indices:\n",
    "            if j == i:\n",
    "                continue\n",
    "            context_word = text[j]\n",
    "            if central_word in vocabulary and context_word in vocabulary:\n",
    "                context_pairs.append(\n",
    "                    (word_to_index[central_word], word_to_index[context_word])\n",
    "                )\n",
    "\n",
    "print(f\"Generated {len(context_pairs)} pairs of target and context words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подзадача №1: subsampling\n",
    "Для того, чтобы сгладить разницу в частоте встречаемсости слов, необходимо реализовать механизм subsampling'а.\n",
    "Для этого вам необходимо реализовать функцию ниже.\n",
    "\n",
    "Вероятность **исключить** слово из обучения (на фиксированном шаге) вычисляется как\n",
    "$$\n",
    "P_\\text{drop}(w_i)=1 - \\sqrt{\\frac{t}{f(w_i)}},\n",
    "$$\n",
    "где $f(w_i)$ – нормированная частота встречаемости слова, а $t$ – заданный порог (threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_frequent_words(word_count_dict, threshold=1e-5):\n",
    "    \"\"\"\n",
    "    Calculates the subsampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function is used to determine the probability of keeping a word in the dataset\n",
    "    when subsampling frequent words. The method used is inspired by the subsampling approach\n",
    "    in Word2Vec, where each word's frequency affects its probability of being kept.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "    - threshold (float, optional): A threshold parameter used to adjust the frequency of word subsampling.\n",
    "                                   Defaults to 1e-5.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of keeping each word.\n",
    "\n",
    "    Example:\n",
    "    >>> word_counts = {'the': 5000, 'is': 1000, 'apple': 50}\n",
    "    >>> subsample_frequent_words(word_counts)\n",
    "    {'the': 0.028, 'is': 0.223, 'apple': 1.0}\n",
    "    \"\"\"\n",
    "\n",
    "    total_words_number = sum(word_count_dict.values())\n",
    "    keep_prob_dict = {}\n",
    "\n",
    "    for word, count in word_count_dict.items():\n",
    "        freq = count / total_words_number\n",
    "        prob = (1.0 + (freq / threshold)**0.5) * (threshold/freq)\n",
    "        keep_prob_dict[word] = min(prob, 1)\n",
    "    return keep_prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.0034906054261852172,\n",
       " 'is': 0.007838674593052023,\n",
       " 'apple': 0.03599505426185218}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsample_frequent_words({'the': 5000, 'is': 1000, 'apple': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подзадача №2: negative sampling\n",
    "Для более эффективного обучения необходимо не только предсказывать высокие вероятности для слов из контекста, но и предсказывать низкие для слов, не встреченных в контексте. Для этого вам необходимо вычислить вероятност использовать слово в качестве negative sample, реализовав функцию ниже.\n",
    "\n",
    "В оригинальной статье предлагается оценивать вероятность слов выступать в качестве negative sample согласно распределению $P_n(w)$\n",
    "$$\n",
    "P_n(w) = \\frac{U(w)^{3/4}}{Z},\n",
    "$$\n",
    "\n",
    "где $U(w)$ распределение слов по частоте (или, как его еще называют, по униграммам), а $Z$ – нормировочная константа, чтобы общая мера была равна $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_prob(word_count_dict):\n",
    "    \"\"\"\n",
    "    Calculates the negative sampling probabilities for words based on their frequencies.\n",
    "\n",
    "    This function adjusts the frequency of each word raised to the power of 0.75, which is\n",
    "    commonly used in algorithms like Word2Vec to moderate the influence of very frequent words.\n",
    "    It then normalizes these adjusted frequencies to ensure they sum to 1, forming a probability\n",
    "    distribution used for negative sampling.\n",
    "\n",
    "    Parameters:\n",
    "    - word_count_dict (dict): A dictionary where keys are words and values are the counts of those words.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words and values are the probabilities of selecting each word\n",
    "            for negative sampling.\n",
    "\n",
    "    Example:\n",
    "    >>> word_counts = {'the': 5000, 'is': 1000, 'apple': 50}\n",
    "    >>> get_negative_sampling_prob(word_counts)\n",
    "    {'the': 0.298, 'is': 0.160, 'apple': 0.042}\n",
    "    \"\"\"\n",
    "\n",
    "    total_sum = 0\n",
    "    negative_sampling_prob_dict = {}\n",
    "\n",
    "    for word, count in word_count_dict.items():\n",
    "        adjusted_count = count**0.75\n",
    "        total_sum += adjusted_count\n",
    "        negative_sampling_prob_dict[word] = adjusted_count\n",
    "\n",
    "    for word in negative_sampling_prob_dict:\n",
    "        negative_sampling_prob_dict[word] /= total_sum\n",
    "\n",
    "    return negative_sampling_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства, преобразуем полученные словари в массивы (т.к. все слова все равно уже пронумерованы)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_dict = subsample_frequent_words(word_count_dict)\n",
    "assert keep_prob_dict.keys() == word_count_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sampling_prob_dict = get_negative_sampling_prob(word_count_dict)\n",
    "assert negative_sampling_prob_dict.keys() == negative_sampling_prob_dict.keys()\n",
    "assert np.allclose(sum(negative_sampling_prob_dict.values()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob_array = np.array(\n",
    "    [keep_prob_dict[index_to_word[idx]] for idx in range(len(word_to_index))]\n",
    ")\n",
    "negative_sampling_prob_array = np.array(\n",
    "    [\n",
    "        negative_sampling_prob_dict[index_to_word[idx]]\n",
    "        for idx in range(len(word_to_index))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_prob_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все прошло успешно, функция ниже поможет вам с генерацией подвыборок (батчей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_with_neg_samples(\n",
    "    context_pairs,\n",
    "    batch_size,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "):\n",
    "    batch = []\n",
    "    neg_samples = []\n",
    "\n",
    "    while len(batch) < batch_size:\n",
    "        center, context = random.choice(context_pairs)\n",
    "        if random.random() < keep_prob_array[center]:\n",
    "            batch.append((center, context))\n",
    "            neg_sample = np.random.choice(\n",
    "                range(len(negative_sampling_prob_array)),\n",
    "                size=num_negatives,\n",
    "                p=negative_sampling_prob_array,\n",
    "                replace = True\n",
    "            )\n",
    "            neg_samples.append(neg_sample)\n",
    "    batch = np.array(batch)\n",
    "    neg_samples = np.vstack(neg_samples)\n",
    "    return batch, neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_negatives = 15\n",
    "batch, neg_samples = generate_batch_with_neg_samples(\n",
    "    context_pairs,\n",
    "    batch_size,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, время реализовать модель. Обращаем ваше внимание, использование линейных слоев (`nn.Linear`) далеко не всегда оправданно!\n",
    "\n",
    "Напомним, что в случае negative sampling решается задача максимизации следующего функционала:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\log \\sigma({\\mathbf{v}'_{w_O}}^\\top \\mathbf{v}_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} \\left[ \\log \\sigma({-\\mathbf{v}'_{w_i}}^\\top \\mathbf{v}_{w_I}) \\right],\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $\\mathbf{v}_{w_I}$ – вектор центрального слова $w_I$,\n",
    "- $\\mathbf{v}'_{w_O}$ – вектор слова из контекста $w_O$,\n",
    "- $k$ – число negative samplesЮ,\n",
    "- $P_n(w)$ – распределение negative samples, заданное выше,\n",
    "- $\\sigma$ – сигмоида."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModelWithNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.center_embeddings =   nn.Parameter(torch.rand(vocab_size, embedding_dim))\n",
    "        self.context_embeddings =   nn.Parameter(torch.rand(vocab_size, embedding_dim))\n",
    "\n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "        # pos_context_words (batch_size, num_negs)\n",
    "        \n",
    "        pos_embs = self.context_embeddings[pos_context_words].unsqueeze(2) #(batch_size, embedding_dim, 1)\n",
    "        cent_embs = self.center_embeddings[center_words].unsqueeze(1) #(batch_size, 1, embedding_dim)\n",
    "        neg_embs = self.context_embeddings[neg_context_words].permute(0, 2, 1) #(batch_size, embedding_dim, num_negs)\n",
    "        \n",
    "        pos_scores = 0  # THIS IS A PLACEHOLDER\n",
    "        neg_scores = 0  # THIS IS A PLACEHOLDER\n",
    "        pos_scores = torch.bmm(cent_embs, pos_embs).squeeze() # batch_size\n",
    "        neg_scores = torch.bmm(cent_embs, neg_embs).squeeze() # batch_size, num_negs\n",
    "        \n",
    "        return pos_scores, neg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 32\n",
    "num_negatives = 15\n",
    "\n",
    "model = SkipGramModelWithNegSampling(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=150)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_counter = 0\n",
    "for weights in model.parameters():\n",
    "    params_counter += weights.shape.numel()\n",
    "assert params_counter == len(word_to_index) * embedding_dim * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_with_neg_sampling(\n",
    "    model,\n",
    "    context_pairs,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    batch_size,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "    steps,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=device,\n",
    "):\n",
    "    pos_labels = torch.ones(batch_size).to(device)\n",
    "    neg_labels = torch.zeros(batch_size, num_negatives).to(device)\n",
    "    loss_history = []\n",
    "    for step in tqdma(range(steps)):\n",
    "        batch, neg_samples = generate_batch_with_neg_samples(\n",
    "            context_pairs,\n",
    "            batch_size,\n",
    "            keep_prob_array,\n",
    "            word_to_index,\n",
    "            num_negatives,\n",
    "            negative_sampling_prob_array,\n",
    "        )\n",
    "        center_words = torch.tensor([pair[0] for pair in batch], dtype=torch.long).to(\n",
    "            device\n",
    "        )\n",
    "        pos_context_words = torch.tensor(\n",
    "            [pair[1] for pair in batch], dtype=torch.long\n",
    "        ).to(device)\n",
    "        neg_context_words = torch.tensor(neg_samples, dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pos_scores, neg_scores = model(\n",
    "            center_words, pos_context_words, neg_context_words\n",
    "        )\n",
    "\n",
    "        loss_pos = criterion(pos_scores, pos_labels)\n",
    "        loss_neg = criterion(neg_scores, neg_labels)\n",
    "\n",
    "        loss = loss_pos + loss_neg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        lr_scheduler.step(loss_history[-1])\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Step {step}, Loss: {np.mean(loss_history[-100:])}, learning rate: {lr_scheduler._last_lr}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13abbcd72764b2abd2d42fbbf6d5c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 7.725884437561035, learning rate: [0.05]\n",
      "Step 100, Loss: 3.4465584683418276, learning rate: [0.05]\n",
      "Step 200, Loss: 2.171271185874939, learning rate: [0.05]\n",
      "Step 300, Loss: 2.0858760750293732, learning rate: [0.05]\n",
      "Step 400, Loss: 2.090978126525879, learning rate: [0.025]\n",
      "Step 500, Loss: 2.0193501043319704, learning rate: [0.025]\n",
      "Step 600, Loss: 1.9887124347686767, learning rate: [0.025]\n",
      "Step 700, Loss: 1.9541540586948394, learning rate: [0.025]\n",
      "Step 800, Loss: 1.9373326098918915, learning rate: [0.0125]\n",
      "Step 900, Loss: 1.914826681613922, learning rate: [0.0125]\n",
      "Step 1000, Loss: 1.877104650735855, learning rate: [0.0125]\n",
      "Step 1100, Loss: 1.8548032653331756, learning rate: [0.0125]\n",
      "Step 1200, Loss: 1.8267393827438354, learning rate: [0.00625]\n",
      "Step 1300, Loss: 1.7968767619132995, learning rate: [0.003125]\n",
      "Step 1400, Loss: 1.77722887635231, learning rate: [0.003125]\n",
      "Step 1500, Loss: 1.7958993136882782, learning rate: [0.0015625]\n",
      "Step 1600, Loss: 1.8113911497592925, learning rate: [0.0015625]\n",
      "Step 1700, Loss: 1.787601488828659, learning rate: [0.00078125]\n",
      "Step 1800, Loss: 1.7753978407382964, learning rate: [0.000390625]\n",
      "Step 1900, Loss: 1.813427152633667, learning rate: [0.000390625]\n",
      "Step 2000, Loss: 1.771366413831711, learning rate: [0.0001953125]\n",
      "Step 2100, Loss: 1.7865438961982727, learning rate: [0.0001953125]\n",
      "Step 2200, Loss: 1.760297943353653, learning rate: [9.765625e-05]\n",
      "Step 2300, Loss: 1.7658272302150726, learning rate: [9.765625e-05]\n",
      "Step 2400, Loss: 1.7549513518810271, learning rate: [4.8828125e-05]\n"
     ]
    }
   ],
   "source": [
    "steps = 2500\n",
    "batch_size = 64\n",
    "train_skipgram_with_neg_sampling(\n",
    "    model,\n",
    "    context_pairs,\n",
    "    keep_prob_array,\n",
    "    word_to_index,\n",
    "    batch_size,\n",
    "    num_negatives,\n",
    "    negative_sampling_prob_array,\n",
    "    steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, используйте полученную матрицу весов в качестве матрицы в векторными представлениями слов. Рекомендуем использовать для сдачи матрицу, которая отвечала за слова из контекста (т.е. декодера)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_parameters = model.parameters()\n",
    "embedding_matrix_center = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that first matrix was for central word\n",
    "embedding_matrix_context = next(\n",
    "    _model_parameters\n",
    ").detach()  # Assuming that second matrix was for context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, embedding_matrix, word_to_index=word_to_index):\n",
    "    return embedding_matrix[word_to_index[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простые проверки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1 = F.cosine_similarity(\n",
    "    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"apple\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "similarity_2 = F.cosine_similarity(\n",
    "    get_word_vector(\"iphone\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"dell\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "assert similarity_1 > similarity_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_1 = F.cosine_similarity(\n",
    "    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"laptop\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "similarity_2 = F.cosine_similarity(\n",
    "    get_word_vector(\"windows\", embedding_matrix_context)[None, :],\n",
    "    get_word_vector(\"macbook\", embedding_matrix_context)[None, :],\n",
    ")\n",
    "assert similarity_1 > similarity_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, взглянем на ближайшие по косинусной мере слова. Функция реализована ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dA0V6rOYtsLk"
   },
   "outputs": [],
   "source": [
    "def find_nearest(word, embedding_matrix, word_to_index=word_to_index, k=10):\n",
    "    word_vector = get_word_vector(word, embedding_matrix)[None, :]\n",
    "    dists = F.cosine_similarity(embedding_matrix, word_vector)\n",
    "    index_sorted = torch.argsort(dists)\n",
    "    top_k = index_sorted[-k:]\n",
    "    return [(index_to_word[x], dists[x].item()) for x in top_k.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('304', 0.7013172507286072),\n",
       " ('pancakes', 0.7079888582229614),\n",
       " ('hardly', 0.7085108160972595),\n",
       " ('soulmates', 0.7098197340965271),\n",
       " ('organ', 0.720969557762146),\n",
       " ('presses', 0.7209898233413696),\n",
       " ('britney', 0.7263519763946533),\n",
       " ('homophobic', 0.7514375448226929),\n",
       " ('writes', 0.781930148601532),\n",
       " ('python', 0.9999999403953552)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest(\"python\", embedding_matrix_context, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также вы можете визуально проверить, как представлены в латентном пространстве часто встречающиеся слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5000\n",
    "_top_words = sorted([x for x in word_count_dict.items()], key=lambda x: x[1])[\n",
    "    -top_k - 100 : -100\n",
    "]  # ignoring 100 most frequent words\n",
    "top_words = [x[0] for x in _top_words]\n",
    "del _top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = torch.cat(\n",
    "    [embedding_matrix_context[word_to_index[x]][None, :] for x in top_words], dim=0\n",
    ").cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"b8e6310a-17bf-4388-baa4-86470c6e222d\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"b8e6310a-17bf-4388-baa4-86470c6e222d\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"b8e6310a-17bf-4388-baa4-86470c6e222d\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"b8e6310a-17bf-4388-baa4-86470c6e222d\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"b8e6310a-17bf-4388-baa4-86470c6e222d\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh.models as bm\n",
    "import bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "\n",
    "def draw_vectors(\n",
    "    x,\n",
    "    y,\n",
    "    radius=10,\n",
    "    alpha=0.25,\n",
    "    color=\"blue\",\n",
    "    width=600,\n",
    "    height=400,\n",
    "    show=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"draws an interactive plot for data points with auxilirary info on hover\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({\"x\": x, \"y\": y, \"color\": color, **kwargs})\n",
    "\n",
    "    fig = pl.figure(active_scroll=\"wheel_zoom\", width=width, height=height)\n",
    "    fig.scatter(\"x\", \"y\", size=radius, color=\"color\", alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show:\n",
    "        pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = umap.UMAP(n_neighbors=5).fit_transform(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_vectors(embedding[:, 0], embedding[:, 1], token=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сдачи задания необходимо загрузить функции `subsample_frequent_words` и `get_negative_sampling_prob`, а также сгенерировать файл для посылки ниже и приложить в соответствующую задачу. Успехов!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "import json\n",
    "\n",
    "assert os.path.exists(\n",
    "    \"words_subset.txt\"\n",
    "), \"Please, download `words_subset.txt` and place it in the working directory\"\n",
    "\n",
    "with open(\"words_subset.txt\") as iofile:\n",
    "    selected_words = iofile.read().split(\"\\n\")\n",
    "\n",
    "\n",
    "def get_matrix_for_selected_words(selected_words, embedding_matrix, word_to_index):\n",
    "    word_vectors = []\n",
    "    for word in selected_words:\n",
    "        index = word_to_index.get(word, None)\n",
    "        vector = [0.0] * embedding_matrix.shape[1]\n",
    "        if index is not None:\n",
    "            vector = embedding_matrix[index].cpu().numpy().tolist()\n",
    "        word_vectors.append(vector)\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "word_vectors = get_matrix_for_selected_words(\n",
    "    selected_words, embedding_matrix_context, word_to_index\n",
    ")\n",
    "\n",
    "with open(\"submission_dict.json\", \"w\") as iofile:\n",
    "    json.dump(word_vectors, iofile)\n",
    "print(\"File saved to `submission_dict.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
