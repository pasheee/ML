{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация FashionMNIST\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/girafe_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В этом задании мы воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша основная задача: реализовать весь пайплайн обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку с первого занятия.\n",
    "\n",
    "Мы настоятельно рекомендуем писать код «с нуля», лишь изредка подглядывая в готовые примеры, а не просто «копировать-вставлять». Это поможет вам в будущем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_mnist_data = MNIST('.', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_mnist_data = MNIST('.', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f'Image label: {_label}')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте модель, представленную ниже. Пожалуйста, не создавайте чрезмерно сложную сеть — она не должна быть глубже четырёх слоёв (можно и меньше). Ваша основная задача — обучить модель и добиться как минимум 92% точности на тестовой выборке (hold-out выборке).\n",
    "\n",
    "*Примечание: линейных слоёв и функций активации должно быть достаточно.*\n",
    "\n",
    "__Обратите внимание, ваша модель должна быть представлена переменной `model`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CV_Classifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.first_linear = nn.Linear(input_size, input_size // 2)\n",
    "        self.non_linear = nn.Tanh()\n",
    "        self.second_linear = nn.Linear(input_size // 2, input_size // 4)\n",
    "        self.projection = nn.Linear(input_size // 4, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        f_out = self.first_linear(X)\n",
    "        s_out = self.second_linear(self.non_linear(f_out))\n",
    "        predictions = self.projection(self.non_linear(s_out))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model instance\n",
    "model = CV_Classifier(28) # your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже доступны локальные тесты для проверки вашей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is wrong with the model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x784 and 28x14)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSomething is wrong with the model\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m y_predicted.shape[-\u001b[32m1\u001b[39m] == \u001b[32m10\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mModel should predict 10 logits/probas\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEverything seems fine!\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     y = random_batch[\u001b[32m1\u001b[39m]\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# compute outputs given inputs, both are variables\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     y_predicted = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSomething is wrong with the model\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mCV_Classifier.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     f_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfirst_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     s_out = \u001b[38;5;28mself\u001b[39m.second_linear(\u001b[38;5;28mself\u001b[39m.non_linear(f_out))\n\u001b[32m     13\u001b[39m     predictions = \u001b[38;5;28mself\u001b[39m.projection(\u001b[38;5;28mself\u001b[39m.non_linear(s_out))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (32x784 and 28x14)"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model is not None, 'Please, use `model` variable to store your model'\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].reshape(-1, 784)\n",
    "    y = random_batch[1]\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model(x)    \n",
    "except Exception as e:\n",
    "    print('Something is wrong with the model')\n",
    "    raise e\n",
    "    \n",
    "    \n",
    "assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
    "\n",
    "print('Everything seems fine!')\n",
    "# __________end of block__________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите модель на обучающей выборке. Рекомендуем поэкспериментировать с различными оптимизаторами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также помните, что вы всегда можете обратиться к отличной [документации](https://pytorch.org/docs/stable/index.html) и [учебным материалам](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "train_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Neural network accuracy on train set: {train_acc:3.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "test_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Neural network accuracy on test set: {test_acc:3.5}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
    "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model`, а файл `hw_mnist_data_dict.npy` находится в той же директории, что и ноутбук (он доступен в репозитории)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "import json\n",
    "assert os.path.exists('hw_mnist_data_dict.npy'), 'Please, download `hw_mnist_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "    \n",
    "    predicted_labels = torch.cat(predicted_labels).numpy()\n",
    "    predicted_labels = ','.join([str(x) for x in list(predicted_labels)])\n",
    "    return predicted_labels\n",
    "\n",
    "loaded_data_dict = np.load('hw_mnist_data_dict.npy', allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    'train': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])),\n",
    "    'test': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['test']))\n",
    "}\n",
    "\n",
    "with open('submission_dict_mnist_task_1.json', 'w') as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print('File saved to `submission_dict_mnist_task_1.json`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Сдайте сгенерированный файл в соответствующую задачу в соревновании, а именно:\n",
    "    \n",
    "* `submission_dict_mnist_task_1.json` в задачу Warmup (hw_mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
