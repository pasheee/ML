{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ADTojbpfLt"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\" width=\"400\"></p>\n",
    "\n",
    "# Домашнее задание. Обучение языковой модели с помощью LSTM (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldHSmYY6p_mZ"
   },
   "source": [
    "Э\n",
    "В этом задании Вам предстоит обучить языковую модель с помощью рекуррентной нейронной сети. В отличие от семинарского занятия, Вам необходимо будет работать с отдельными словами, а не буквами.\n",
    "\n",
    "\n",
    "Установим модуль ```datasets```, чтобы нам проще было работать с данными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh9ZXSeCpng9"
   },
   "source": [
    "Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XOJi16bLpd_O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import seaborn\n",
    "seaborn.set(palette='summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "91JuM0SQvXud"
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "adJC8ShFq9HM"
   },
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "device = torch.device(\"cuda\")"
=======
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwsfS1ENq5ig"
   },
   "source": [
    "## Подготовка данных\n",
    "\n",
    "Воспользуемся датасетом imdb. В нем хранятся отзывы о фильмах с сайта imdb. Загрузим данные с помощью функции ```load_dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qHLNWOfJqSfc"
   },
   "outputs": [],
   "source": [
    "# Загрузим датасет\n",
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24gn7CuZ9agP"
   },
   "source": [
    "### Препроцессинг данных и создание словаря (1 балл)\n",
    "\n",
    "Далее вам необходмо самостоятельно произвести препроцессинг данных и получить словарь или же просто ```set``` строк. Что необходимо сделать:\n",
    "\n",
    "1. Разделить отдельные тренировочные примеры на отдельные предложения с помощью функции ```sent_tokenize``` из бибилиотеки ```nltk```. Каждое отдельное предложение будет одним тренировочным примером.\n",
    "2. Оставить только те предложения, в которых меньше ```word_threshold``` слов.\n",
    "3. Посчитать частоту вхождения каждого слова в оставшихся предложениях. Для деления предлоения на отдельные слова удобно использовать функцию ```word_tokenize```.\n",
    "4. Создать объект ```vocab``` класса ```set```, положить в него служебные токены '\\<unk\\>', '\\<bos\\>', '\\<eos\\>', '\\<pad\\>' и vocab_size самых частовстречающихся слов.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ins2tVCdsS47"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "word_threshold = 32\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "for sample in train_dataset['text']:\n",
    "    sentences.extend([sentence for sentence in sent_tokenize(sample) if len(sentence.split()) < word_threshold])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bxeBxP3J1Rj3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего предложений: 219930\n"
     ]
    }
   ],
   "source": [
    "print(\"Всего предложений:\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT82XkT6ULA_"
   },
   "source": [
    "Посчитаем для каждого слова его встречаемость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nEvCN0Y1w1yH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< Updated upstream
       "model_id": "0a0d9dabfacc4e52843b09c97eaee833",
=======
       "model_id": "bd15741737354fc684969e4e481d3b05",
>>>>>>> Stashed changes
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/219930 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = Counter()\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    for word in word_tokenize(sentence):\n",
    "        words[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92516"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4k4uSoHUSI0"
   },
   "source": [
    "Добавим в словарь ```vocab_size``` самых встречающихся слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oUBNwsK9xLIu"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "vocab_size = 40000\n",
    "vocab.update(['<unk>', '<bos>', '<eos>', '<pad>'])\n",
    "count = 0\n",
    "for word in words.keys():\n",
    "    vocab.add(word)\n",
    "    count+=1\n",
    "    if count == 40000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ieT0DFUpXnV2"
   },
   "outputs": [],
   "source": [
    "assert '<unk>' in vocab\n",
    "assert '<bos>' in vocab\n",
    "assert '<eos>' in vocab\n",
    "assert '<pad>' in vocab\n",
    "assert len(vocab) == vocab_size + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JhACW2CQyck5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов в словаре: 40004\n"
     ]
    }
   ],
   "source": [
    "print(\"Всего слов в словаре:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmeRYKSIUcdE"
   },
   "source": [
    "### Подготовка датасета (1 балл)\n",
    "\n",
    "Далее, как и в семинарском занятии, подготовим датасеты и даталоадеры.\n",
    "\n",
    "В классе ```WordDataset``` вам необходимо реализовать метод ```__getitem__```, который будет возвращать сэмпл данных по входному idx, то есть список целых чисел (индексов слов).\n",
    "\n",
    "Внутри этого метода необходимо добавить служебные токены начала и конца последовательности, а также токенизировать соответствующее предложение с помощью ```word_tokenize``` и сопоставить ему индексы из ```word2ind```."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
>>>>>>> Stashed changes
   "metadata": {
    "id": "iD7SmSy3v2dl"
   },
   "outputs": [],
   "source": [
    "word2ind = {char: i for i, char in enumerate(vocab)}\n",
    "ind2word = {i: char for char, i in word2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
=======
   "execution_count": 14,
>>>>>>> Stashed changes
   "metadata": {
    "id": "FVzXL17PzC7K"
   },
   "outputs": [],
   "source": [
    "class WordDataset:\n",
    "    def __init__(self, sentences):\n",
    "        self.data = sentences\n",
    "        # self.unk_id = word2ind['<unk>']\n",
    "        # self.bos_id = word2ind['<bos>']\n",
    "        # self.eos_id = word2ind['<eos>']\n",
    "        # self.pad_id = word2ind['<pad>']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Simulate some data retrieval\n",
    "        return torch.randn(10)  \n",
    "\n",
    "    def __len__(self) -> int:\n",
<<<<<<< Updated upstream
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "I6CtYNMp2_g0"
   },
   "outputs": [],
   "source": [
    "def collate_fn_with_padding(\n",
    "    input_batch: List[List[int]], pad_id=word2ind['<pad>']) -> torch.Tensor:\n",
    "    seq_lens = [len(x) for x in input_batch]\n",
    "    max_seq_len = max(seq_lens)\n",
    "\n",
    "    new_batch = []\n",
    "    for sequence in input_batch:\n",
    "        for _ in range(max_seq_len - len(sequence)):\n",
    "            sequence.append(pad_id)\n",
    "        new_batch.append(sequence)\n",
    "\n",
    "    sequences = torch.LongTensor(new_batch).to(device)\n",
    "\n",
    "    new_batch = {\n",
    "        'input_ids': sequences[:,:-1],\n",
    "        'target_ids': sequences[:,1:]\n",
    "    }\n",
    "\n",
    "    return new_batch"
=======
    "        return len(self.data)\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "I6CtYNMp2_g0"
   },
   "outputs": [],
   "source": [
    "def collate_fn_with_padding(input_batch: List[List[int]], pad_id=word2ind['<pad>']) -> Dict[str, torch.Tensor]:\n",
    "    try:\n",
    "        print(f\"Collating batch (PID: {os.getpid()}) - Batch size: {len(input_batch)}\") # Print batch size and PID\n",
    "        seq_lens = [len(x) for x in input_batch]\n",
    "        max_seq_len = max(seq_lens)\n",
    "\n",
    "        padded_batch = [sequence + [pad_id] * (max_seq_len - len(sequence)) for sequence in input_batch]\n",
    "\n",
    "        sequences = torch.LongTensor(padded_batch)\n",
    "\n",
    "        print(f\"Finished collating batch (PID: {os.getpid()})\")\n",
    "        return {\n",
    "            'input_ids': sequences[:, :-1],\n",
    "            'target_ids': sequences[:, 1:]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate_fn: {e}\")\n",
    "        raise # Re-raise after logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6xmeK9Ys1BIG"
   },
   "outputs": [],
   "source": [
    "train_sentences, eval_sentences = train_test_split(sentences, test_size=0.2)\n",
    "eval_sentences, test_sentences = train_test_split(sentences, test_size=0.5)\n",
    "\n",
    "train_dataset = WordDataset(train_sentences)\n",
    "eval_dataset = WordDataset(eval_sentences)\n",
    "test_dataset = WordDataset(test_sentences)\n",
    "\n",
<<<<<<< Updated upstream
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
=======
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size, num_workers=1)\n",
>>>>>>> Stashed changes
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size,  num_workers = 1)\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMAexY7Y45E4"
   },
   "source": [
    "## Обучение и архитектура модели\n",
    "\n",
    "Вам необходимо на практике проверить, что влияет на качество языковых моделей. В этом задании нужно провести серию экспериментов с различными вариантами языковых моделей и сравнить различия в конечной перплексии на тестовом множестве.\n",
    "\n",
    "Возмоэные идеи для экспериментов:\n",
    "\n",
    "* Различные RNN-блоки, например, LSTM или GRU. Также можно добавить сразу несколько RNN блоков друг над другом с помощью аргумента num_layers. Вам поможет официальная документация [здесь](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "* Различные размеры скрытого состояния. Различное количество линейных слоев после RNN-блока. Различные функции активации.\n",
    "* Добавление нормализаций в виде Dropout, BatchNorm или LayerNorm\n",
    "* Различные аргументы для оптимизации, например, подбор оптимального learning rate или тип алгоритма оптимизации SGD, Adam, RMSProp и другие\n",
    "* Любые другие идеи и подходы\n",
    "\n",
    "После проведения экспериментов необходимо составить таблицу результатов, в которой описан каждый эксперимент и посчитана перплексия на тестовом множестве.\n",
    "\n",
    "Учтите, что эксперименты, которые различаются, например, только размером скрытого состояния или количеством линейных слоев считаются, как один эксперимент.\n",
    "\n",
    "Успехов!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP1cO-3bmDv9"
   },
   "source": [
    "### Функция evaluate (1 балл)\n",
    "\n",
    "Заполните функцию ```evaluate```"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 26,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    class LanguageModel(nn.Module):\n",
    "        def __init__(self, hidden_dim: int, vocab_size: int):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "            self.rnn = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
    "            self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "            self.projection = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "            self.non_lin = nn.Tanh()\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "    \n",
    "        def forward(self, input_batch) -> torch.Tensor:\n",
    "            embeddings = self.embedding(input_batch)  # [batch_size, seq_len, hidden_dim]\\\n",
    "            output, _ = self.rnn(embeddings)  # [batch_size, seq_len, hidden_dim]\n",
    "            output = self.dropout(self.linear(self.non_lin(output)))  # [batch_size, seq_len, hidden_dim]\n",
    "            projection = self.projection(self.non_lin(output))  # [batch_size, seq_len, vocab_size]\n",
    "    \n",
    "            return projection\n",
    "\n",
    "    model = LanguageModel(hidden_dim=256, vocab_size=len(vocab))\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    batch_size = 1\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=1)\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, batch_size=batch_size,  num_workers = 1)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    perplexities = []\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        epoch_losses = []\n",
    "        print(1)\n",
    "        for batch in train_dataloader:\n",
    "            print(2)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "    \n",
    "            logits = model(input_ids)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            target_ids = target_ids.flatten()\n",
    "            \n",
    "            loss = criterion(logits, target_ids)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "        perplexities.append(evaluate(model, criterion, eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {
    "id": "XUlMUVJ3mL4r"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader, device='cpu') -> float:\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
<<<<<<< Updated upstream
    "        for batch in tqdm(dataloader):\n",
    "            logits = model(batch['input_ids'].to(device)).flatten(start_dim=0, end_dim=1)\n",
    "            loss = criterion(logits, batch['target_ids'].to(device).flatten())\n",
    "            perplexity.append(torch.exp(loss).item())\n",
    "\n",
    "    perplexity = sum(perplexity) / len(perplexity)\n",
=======
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
>>>>>>> Stashed changes
    "\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            target_ids = target_ids.flatten()\n",
    "            \n",
    "            loss = criterion(logits, target_ids)\n",
    "            \n",
    "            batch_size = target_ids.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLV63Vsk7loy"
   },
   "source": [
    "### Train loop (1 балл)\n",
    "\n",
    "Напишите функцию для обучения модели."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {
    "id": "bSZmUC3YmocP"
   },
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "def train_model(model, criterion, train_dataloader, eval_dataloader, optimizer, num_epoch):\n",
=======
    "def train_model(model, criterion, train_dataloader, eval_dataloader, optimizer, num_epoch, device = 'cpu'):\n",
>>>>>>> Stashed changes
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    perplexities = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_losses = []\n",
    "        print(1)\n",
    "        for batch in train_dataloader:\n",
    "            print(2)\n",
    "            optimizer.zero_grad()\n",
<<<<<<< Updated upstream
    "            logits = model(batch['input_ids'].to(device)).flatten(start_dim=0, end_dim=1)\n",
    "            loss = criterion(\n",
    "                logits, batch['target_ids'].to(device).flatten())\n",
=======
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            target_ids = target_ids.flatten()\n",
    "            \n",
    "            loss = criterion(logits, target_ids)\n",
>>>>>>> Stashed changes
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            epoch_losses.append(loss.item())\n",
    "        \n",
    "        losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "        perplexities.append(evaluate(model, criterion, eval_dataloader))\n",
    "            \n",
    "    return losses, perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_perplexity(per):\n",
    "    plt.plot(per)\n",
    "    plt.show()\n",
    "def plotting_losses(loss):\n",
    "    plt.plot(loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXmeyhBQmuq4"
   },
   "source": [
    "### Первый эксперимент (2 балла)\n",
    "\n",
    "Определите архитектуру модели и обучите её."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 30,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {
    "id": "qaWvqNJom0ij"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.rnn = nn.RNN(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.projection = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.non_lin = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input_batch) -> torch.Tensor:\n",
    "        embeddings = self.embedding(input_batch)  # [batch_size, seq_len, hidden_dim]\\\n",
    "        output, _ = self.rnn(embeddings)  # [batch_size, seq_len, hidden_dim]\n",
    "        output = self.dropout(self.linear(self.non_lin(output)))  # [batch_size, seq_len, hidden_dim]\n",
    "        projection = self.projection(self.non_lin(output))  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        return projection"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 31,
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "metadata": {
    "id": "TxbEzn5fnBOY"
   },
   "outputs": [],
   "source": [
    "model = LanguageModel(hidden_dim=256, vocab_size=len(vocab))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec01779343c412fa0e645dbe6899833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate(model, criterion, eval_dataloader)\n\u001b[1;32m      2\u001b[0m losses, perplexities \u001b[38;5;241m=\u001b[39m train_model(model, criterion, train_dataloader, eval_dataloader, optimizer, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, dataloader)\u001b[0m\n\u001b[1;32m      6\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(logits, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m----> 8\u001b[0m         perplexity\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mexp(loss)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     10\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(perplexity) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(perplexity)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(model, criterion, eval_dataloader)\n",
    "losses, perplexities = train_model(model, criterion, train_dataloader, eval_dataloader, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(perplexities)\n",
    "plt.show()"
=======
   "outputs": [],
   "source": [
    "model.train()\n",
    "losses = []\n",
    "perplexities = []\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_losses = []\n",
    "    print(1)\n",
    "    for batch in train_dataloader:\n",
    "        print(2)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        target_ids = target_ids.flatten()\n",
    "        \n",
    "        loss = criterion(logits, target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    losses.append(sum(epoch_losses) / len(epoch_losses))\n",
    "    perplexities.append(evaluate(model, criterion, eval_dataloader))"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, perplexities = train_model(model, criterion, train_dataloader, eval_dataloader, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1EW4faIm0tl"
   },
   "source": [
    "### Второй эксперимент (2 балла)\n",
    "\n",
    "Попробуйте что-то поменять в модели или в пайплайне обучения, идеи для экспериментов можно подсмотреть выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkSE4jR1XzTg"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
<<<<<<< Updated upstream
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, dropout = 0.1)\n",
=======
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
>>>>>>> Stashed changes
    "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.projection = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.non_lin = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input_batch) -> torch.Tensor:\n",
    "        embeddings = self.embedding(input_batch)  # [batch_size, seq_len, hidden_dim]\\\n",
    "        output, _ = self.rnn(embeddings)  # [batch_size, seq_len, hidden_dim]\n",
    "        output = self.dropout(self.linear(self.non_lin(output)))  # [batch_size, seq_len, hidden_dim]\n",
    "        projection = self.projection(self.non_lin(output))  # [batch_size, seq_len, vocab_size]\n",
    "\n",
<<<<<<< Updated upstream
    "        return projection\n",
    "\n",
    "model = LanguageModel(hidden_dim=256, vocab_size=len(vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2ind['<pad>'])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "losses, perplexities = train_model(model, criterion, train_dataloader, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(perplexities)\n",
    "plt.show()"
=======
    "        return projection"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5V9H3eoFeAu"
   },
   "source": [
    "### Отчет (2 балла)\n",
    "\n",
    "Опишите проведенные эксперименты. Сравните перплексии полученных моделей. Предложите идеи по улучшению качества моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2GCDVeeF8LP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
