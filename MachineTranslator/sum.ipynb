{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c30d7f-372c-4f08-ab0e-61cc79988a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb77ce2a-49a2-456a-8d45-897580015e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import read_json, write_json, translate\n",
    "\n",
    "# Пример данных\n",
    "data = \n",
    "\n",
    "# Токенизаторы\n",
    "src_tokenizer = get_tokenizer('basic_english')\n",
    "dst_tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Функции для создания итераторов\n",
    "def yield_tokens(data_iter, language):\n",
    "    for data_sample in data_iter:\n",
    "        yield src_tokenizer(data_sample[language])\n",
    "\n",
    "# Создание вокабуляров\n",
    "src_vocab = build_vocab_from_iterator(yield_tokens(data, 'src'), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "dst_vocab = build_vocab_from_iterator(yield_tokens(data, 'dst'), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "def data_process(data):\n",
    "    src_tensor = []\n",
    "    dst_tensor = []\n",
    "    for sample in data:\n",
    "        src_tensor.append([src_vocab[\"<bos>\"]] + [src_vocab[token] for token in src_tokenizer(sample['src'])] + [src_vocab[\"<eos>\"]])\n",
    "        dst_tensor.append([dst_vocab[\"<bos>\"]] + [dst_vocab[token] for token in dst_tokenizer(sample['dst'])] + [dst_vocab[\"<eos>\"]])\n",
    "    return src_tensor, dst_tensor\n",
    "\n",
    "src_tensor, dst_tensor = data_process(data)\n",
    "\n",
    "# Паддинг\n",
    "def generate_batch(data_batch):\n",
    "    src_batch, dst_batch = [], []\n",
    "    for (src_item, dst_item) in data_batch:\n",
    "        src_batch.append(torch.tensor(src_item, dtype=torch.long))\n",
    "        dst_batch.append(torch.tensor(dst_item, dtype=torch.long))\n",
    "    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"])\n",
    "    dst_batch = nn.utils.rnn.pad_sequence(dst_batch, padding_value=dst_vocab[\"<pad>\"])\n",
    "    return src_batch, dst_batch\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_tensor, dst_tensor):\n",
    "        self.src_tensor = src_tensor\n",
    "        self.dst_tensor = dst_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_tensor[idx], self.dst_tensor[idx]\n",
    "\n",
    "dataset = TranslationDataset(src_tensor, dst_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=generate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e89269-4a67-4f74-b9f7-4d9edbb825e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, trg.shape[1], trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b75202-d95e-4621-a446-3f4edd68a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(dst_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dst_vocab[\"<pad>\"])\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b6666-44f4-4e09-81f1-ad85f82f743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = [src_vocab[\"<bos>\"]] + [src_vocab[token] for token in src_tokenizer(sentence)] + [src_vocab[\"<eos>\"]]\n",
    "    src_indexes = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    outputs = [dst_vocab[\"<bos>\"]]\n",
    "    for i in range(max_len):\n",
    "        trg_indexes = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(src_indexes, trg_indexes, 0)\n",
    "        best_guess = output.argmax(2)[-1].item()\n",
    "        outputs.append(best_guess)\n",
    "        if best_guess == dst_vocab[\"<eos>\"]:\n",
    "            break\n",
    "    translated_sentence = [dst_vocab.itos[idx] for idx in outputs]\n",
    "    return translated_sentence[1:-1]\n",
    "\n",
    "# Пример использования\n",
    "sentence = \"◄▴◓◠▨ ◨▽◠▦◈◬◓▪▼◬▵\"\n",
    "translation = translate_sentence(sentence, src_vocab, dst_vocab, model, device)\n",
    "print(f\"Translated sentence: {' '.join(translation)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
