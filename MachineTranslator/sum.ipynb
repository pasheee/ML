{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88b8e37-1c62-4b3e-bc87-4cdd0f9facb4",
   "metadata": {},
   "source": [
    "МОДЕЛЬ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cadfa7e-01bf-4232-9880-bec80016c7c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTranslatorv3\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_vocab_size, target_vocab_size, sp, tp, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[0;32m      3\u001b[0m                  num_encoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, num_decoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Translatorv3, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Translator(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, sp, tp, hidden_dim=512, n_heads=8, \n",
    "                 num_encoder_layers=6, num_decoder_layers=6, dropout=0.1):\n",
    "        super(Translatorv3, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.target_embeddings = nn.Embedding(target_vocab_size, hidden_dim)\n",
    "        self.source_embeddings = nn.Embedding(source_vocab_size, hidden_dim)\n",
    "        \n",
    "        self.sp = sp\n",
    "        self.tp = tp\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim, \n",
    "            nhead=n_heads, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.projection = nn.Linear(hidden_dim * 2, target_vocab_size)\n",
    "        \n",
    "        self.non_lin = nn.ReLU()\n",
    "        self.normalization = nn.LayerNorm(hidden_dim * 2)\n",
    "\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask.bool() \n",
    "\n",
    "    def create_padding_mask(self, seq, pad_idx):\n",
    "        return (seq == pad_idx).bool() \n",
    "\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "\n",
    "        target_embeddings = self.target_embeddings(target)\n",
    "        source_embeddings = self.source_embeddings(source)\n",
    "        \n",
    "\n",
    "        tgt_seq_len = target.size(1)\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len).to(target.device)\n",
    "        src_key_padding_mask = self.create_padding_mask(source, self.sp).to(source.device)\n",
    "        tgt_key_padding_mask = self.create_padding_mask(target, self.tp).to(target.device)\n",
    "        \n",
    "        output = self.transformer(\n",
    "            src=source_embeddings,\n",
    "            tgt=target_embeddings,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "\n",
    "\n",
    "        output = self.non_lin(self.normalization(self.linear(output)))\n",
    "        projection = self.projection(output)\n",
    "        \n",
    "        return projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4f4f7-362d-4d7e-b36a-c964f68cea92",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6321959f-7a7b-4672-b48a-b13af1368b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m read_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m val_data \u001b[38;5;241m=\u001b[39m read_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m _, _, _, _, _, eval_dataset \u001b[38;5;241m=\u001b[39m make_wordinddicts(val_data, tokenizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_json' is not defined"
     ]
    }
   ],
   "source": [
    "data = read_json('train')\n",
    "val_data = read_json('val')\n",
    "_, _, _, _, _, eval_dataset = make_wordinddicts(val_data, tokenizer)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "source_word2ind, source_ind2word, target_word2ind, target_ind2word, max_len, dataset = make_wordinddicts(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "model = Translatorv3(len(source_word2ind), len(target_word2ind), source_word2ind['<PAD>'], target_word2ind['<PAD>']).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=target_word2ind['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay = 0.00001)\n",
    "\n",
    "\n",
    "train_losses, val_losses = train_model(model, criterion, optimizer, dataloader, eval_dataloader, 5)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "from utils import translate\n",
    "from tqdm import tqdm\n",
    "data = read_json('test_no_reference')\n",
    "result = []\n",
    "for line in tqdm(data):\n",
    "    sentence = [char for char in line['src']]\n",
    "    translated_sentence = translate(model, sentence, source_word2ind, target_word2ind)\n",
    "    # print(translated_sentence)\n",
    "    result.append({'src': line['src'],\n",
    "                   'dst': ''.join(translated_sentence)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953e471-0c4a-411f-a86b-821397c2f82a",
   "metadata": {},
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5632dc-d72f-4819-b6d9-e0f3a96d98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from WordDataset import WordDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "MAX_LEN = 10\n",
    "\n",
    "\n",
    "def read_json(filepath):\n",
    "    try:\n",
    "        data = []\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                data.append(item)\n",
    "        return data\n",
    "        \n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error reading or parsing the file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloader, eval_dataloader, num_epoch, device=torch.device('cuda')):\n",
    "    avg_losses_train = []\n",
    "    avg_losses_val = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        print(f'Epoch: {epoch}')\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for source, target, src_mask, tgt_mask in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Подготовка данных\n",
    "            source, target_input = source.to(device), target[:, :-1].to(device)\n",
    "            target_output = target[:, 1:].to(device).flatten()\n",
    "\n",
    "            src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
    "\n",
    "            # Прямой проход\n",
    "            output = model(source, target_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "    \n",
    "            # Вычисление ошибки\n",
    "            loss = criterion(output, target_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_losses_train.append(avg_train_loss)\n",
    "        print(f'Average train loss: {avg_train_loss:.4f}')\n",
    "        \n",
    "        # Оценка на валидации\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for source, target, src_mask, tgt_mask in tqdm(eval_dataloader):\n",
    "                source, target_input = source.to(device), target[:, :-1].to(device)\n",
    "                target_output = target[:, 1:].to(device).flatten()\n",
    "\n",
    "                src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
    "\n",
    "                output = model(source, target_input)\n",
    "                output = output.view(-1, output.size(-1))\n",
    "                loss = criterion(output, target_output)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_losses_val.append(avg_val_loss)\n",
    "        print(f'Average val loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Сохранение лучшей модели\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print('Model saved.')\n",
    "\n",
    "    return avg_losses_train, avg_losses_val\n",
    "\n",
    "\n",
    "def make_wordinddicts(data, tokenizer):\n",
    "    source = []\n",
    "    target = []\n",
    "    \n",
    "    for line in data:\n",
    "        t, s = line.keys()\n",
    "        target.append(line[t].lower())\n",
    "        source.append(line[s].lower())\n",
    "    \n",
    "    target_bag_of_words = []\n",
    "    source_bag_of_words = []\n",
    "    \n",
    "    target_sentences = []\n",
    "    source_sentences = []\n",
    "    \n",
    "    for i in range(len(target)):\n",
    "        t_sent = target[i]\n",
    "        s_sent = source[i]\n",
    "        t_tokens = tokenizer.tokenize(t_sent.lower())\n",
    "        s_tokens = [char for char in s_sent]\n",
    "        \n",
    "        target_bag_of_words.extend(t_tokens)\n",
    "        source_bag_of_words.extend(s_tokens)\n",
    "    \n",
    "        target_sentences.append(t_tokens)\n",
    "        source_sentences.append(s_tokens)\n",
    "        \n",
    "    \n",
    "    special_symbols = ['<SOS>', '<EOS>', '<PAD>', '<UNK>']\n",
    "    \n",
    "    target_bag_of_words.extend(special_symbols)\n",
    "    source_bag_of_words.extend(special_symbols)\n",
    "    target_bag_of_words = set(target_bag_of_words)\n",
    "    source_bag_of_words = set(source_bag_of_words)\n",
    "    \n",
    "    source_word2ind = {word: ind for ind, word in enumerate(source_bag_of_words)}\n",
    "    target_word2ind = {word: ind for ind, word in enumerate(target_bag_of_words)}\n",
    "    source_ind2word = {ind: word for ind, word in enumerate(source_bag_of_words)}\n",
    "    target_ind2word = {ind: word for ind, word in enumerate(target_bag_of_words)}\n",
    "\n",
    "    max_len = max(max([len(sentence) for sentence in target_sentences]), max([len(sentence) for sentence in source_sentences]))\n",
    "\n",
    "    dataset = WordDataset(source_sentences, target_sentences, source_word2ind, target_word2ind, max_len = MAX_LEN)\n",
    "\n",
    "    return source_word2ind, source_ind2word, target_word2ind, target_ind2word, max_len, dataset\n",
    "\n",
    "\n",
    "\n",
    "def translate(model, sentence, source_word2ind, target_word2ind, device='cuda', max_length=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Переводит предложение, используя модель Translatorv3.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    source_ids = torch.tensor([[source_word2ind.get(word, source_word2ind['<UNK>']) \n",
    "                               for word in sentence]]).to(device)\n",
    "    target_ids = torch.tensor([[target_word2ind['<SOS>']]]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(source_ids, target_ids)\n",
    "            next_word_id = output[0, -1].argmax().item()\n",
    "            \n",
    "            target_ids = torch.cat([target_ids, \n",
    "                                  torch.tensor([[next_word_id]]).to(device)], dim=1)\n",
    "            \n",
    "            if next_word_id == target_word2ind['<EOS>']:\n",
    "                break\n",
    "    \n",
    "    target_ind2word = {v: k for k, v in target_word2ind.items()}\n",
    "    translated = [target_ind2word[idx.item()] for idx in target_ids[0][1:-1]]\n",
    "    \n",
    "    return ' '.join(translated)\n",
    "\n",
    "\n",
    "def write_json(data):\n",
    "    with open('output.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json_line = json.dumps(item, ensure_ascii=False)\n",
    "            f.write(json_line + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d30466-daee-44d4-bb9a-e4d4a83e27e1",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c662c-f825-4198-b6a5-0f708cec3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    def __init__(self, source, target, source_word2ind, target_word2ind, max_len=50):\n",
    "        self.source_samples = source\n",
    "        self.target_samples = target\n",
    "\n",
    "        self.source_word2ind = source_word2ind\n",
    "        self.target_word2ind = target_word2ind\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_samples)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sentence = self.source_samples[idx][:self.max_len]\n",
    "        target_sentence = self.target_samples[idx][:self.max_len]\n",
    "        \n",
    "        source_indices = [self.source_word2ind['<SOS>']] + \\\n",
    "                         [self.source_word2ind.get(word, self.source_word2ind['<UNK>']) for word in source_sentence] + \\\n",
    "                         [self.source_word2ind['<EOS>']]\n",
    "        \n",
    "        target_indices = [self.target_word2ind['<SOS>']] + \\\n",
    "                         [self.target_word2ind.get(word, self.target_word2ind['<UNK>']) for word in target_sentence] + \\\n",
    "                         [self.target_word2ind['<EOS>']]\n",
    "        \n",
    "        source_indices += [self.source_word2ind['<PAD>']] * (self.max_len + 2 - len(source_indices))\n",
    "        target_indices += [self.target_word2ind['<PAD>']] * (self.max_len + 2 - len(target_indices))\n",
    "\n",
    "        src_padding_mask = torch.tensor([token != self.source_word2ind['<PAD>'] for token in source_indices], dtype=torch.bool)\n",
    "        tgt_padding_mask = torch.tensor([token != self.target_word2ind['<PAD>'] for token in target_indices], dtype=torch.bool)\n",
    "        tgt_padding_mask = tgt_padding_mask[1:]\n",
    "        \n",
    "        source_tensor = torch.tensor(source_indices, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target_indices, dtype=torch.long)\n",
    "    \n",
    "        return source_tensor, target_tensor, src_padding_mask, tgt_padding_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
