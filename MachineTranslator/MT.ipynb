{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412c94f4-c7f6-46dc-82b9-104125ef8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from training import train_transformer\n",
    "from data_preprocessing import make_wordinddicts\n",
    "from utils import read_json, write_json, translate\n",
    "from Translator import Translator, Translatorv2, Translatorv3\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\b\\w+(?:'\\w+)?\\b\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81996a38-823a-406d-a6ed-b0434fb1ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, ignore_index=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: [batch_size, seq_len, n_classes] — логиты модели.\n",
    "            target: [batch_size, seq_len] — индексы целевых классов.\n",
    "        \"\"\"\n",
    "        log_prob = nn.functional.log_softmax(pred, dim=-1)\n",
    "        n_classes = pred.size(-1)\n",
    "\n",
    "        # Создаем true distribution\n",
    "        true_dist = torch.zeros_like(pred)\n",
    "        true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "        true_dist.scatter_(2, target.unsqueeze(2), 1.0 - self.smoothing)\n",
    "\n",
    "        # Маскируем позиции с ignore_index\n",
    "        mask = (target != self.ignore_index).unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "        log_prob = log_prob * mask  # Маскируем логарифмы\n",
    "        true_dist = true_dist * mask  # Маскируем распределение\n",
    "\n",
    "        # Нормализация true_dist (восстановление корректного распределения)\n",
    "        true_dist = true_dist / true_dist.sum(dim=-1, keepdim=True).clamp_min(1e-12)\n",
    "\n",
    "        # Вычисляем потери (усреднение по валидным токенам)\n",
    "        loss = (-true_dist * log_prob).sum(dim=-1)  # Суммируем по классам\n",
    "        loss = loss.sum() / mask.sum()  # Учитываем только валидные элементы\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bba4f77-40d4-46fe-9d13-dcd3d4708bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_json('train')\n",
    "val_data = read_json('val')\n",
    "\n",
    "\n",
    "source_word2ind, source_ind2word, target_word2ind, target_ind2word, source_max_len, target_max_len, dataset = make_wordinddicts(data, tokenizer)\n",
    "_, _, _, _, _, _,  eval_dataset = make_wordinddicts(val_data, tokenizer)\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0776209c-4bb3-4238-ba2e-adbefbfb11ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 66\n"
     ]
    }
   ],
   "source": [
    "print(source_max_len, target_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965ed56c-250b-45ac-ab93-745089be96f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m sp \u001b[38;5;241m=\u001b[39m source_word2ind[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m tp \u001b[38;5;241m=\u001b[39m target_word2ind[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m Translatorv3(\u001b[38;5;28mlen\u001b[39m(source_word2ind), \u001b[38;5;28mlen\u001b[39m(target_word2ind), sp, tp, num_encoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, num_decoder_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m , n_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index \u001b[38;5;241m=\u001b[39m tp)\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "File \u001b[1;32mF:\\Anaconda\\envs\\newenvpls\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32mF:\\Anaconda\\envs\\newenvpls\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Anaconda\\envs\\newenvpls\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Anaconda\\envs\\newenvpls\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1327\u001b[0m         device,\n\u001b[0;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m         non_blocking,\n\u001b[0;32m   1330\u001b[0m     )\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mF:\\Anaconda\\envs\\newenvpls\\Lib\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "sp = source_word2ind['<PAD>']\n",
    "tp = target_word2ind['<PAD>']\n",
    "\n",
    "model = Translatorv3(len(source_word2ind), len(target_word2ind), sp, tp, num_encoder_layers=6, num_decoder_layers=6, hidden_dim = 512, dropout=0.4 , n_heads = 8).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tp)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fc442-93dc-4db2-a12c-bde311eb2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_transformer(model, criterion, optimizer, scheduler, dataloader, eval_dataloader, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c279919-6bf7-42a3-a143-0f2727d644a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = 'val')\n",
    "plt.plot(val_losses, label = 'val')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a957399-05ad-4ddc-ab91-e68c4edbbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для ускорения самообучения ии помоги ребятам написать программу её задача проверять является ли "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56a2a8-1d73-4813-934c-d4b17d89dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d320caf-dcaa-4e0a-989d-4b8bfd32502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import translate\n",
    "from tqdm import tqdm\n",
    "data = read_json('test_no_reference')\n",
    "result = []\n",
    "for line in tqdm(data):\n",
    "    sentence = [char for char in line['src']]\n",
    "    translated_sentence = translate(model, sentence, source_word2ind, target_word2ind)\n",
    "    # print(translated_sentence)\n",
    "    result.append({'src': line['src'],\n",
    "                   'dst': ''.join(translated_sentence)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03332e-0501-41f9-9392-1457670b43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17baca77-909c-4a18-82b2-14da72b1859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from training import train_transformer\n",
    "# from data_preprocessing import make_wordinddicts\n",
    "# from utils import read_json, write_json, translate\n",
    "# from Translator import Translator, Translatorv2, Translatorv3\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# tokenizer = RegexpTokenizer(r\"\\b\\w+(?:'\\w+)?\\b\")\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 500\n",
    "# data = read_json('train')\n",
    "# val_data = read_json('val')\n",
    "\n",
    "\n",
    "# source_word2ind, source_ind2word, target_word2ind, target_ind2word, max_len, dataset = make_wordinddicts(data, tokenizer)\n",
    "# _, _, _, _, _, eval_dataset = make_wordinddicts(val_data, tokenizer)\n",
    "\n",
    "\n",
    "# eval_dataloader = DataLoader(eval_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "# sp = source_word2ind['<PAD>']\n",
    "# tp = target_word2ind['<PAD>']\n",
    "\n",
    "# model = Translatorv3(len(source_word2ind), len(target_word2ind), sp, tp, num_encoder_layers=1, num_decoder_layers=1, hidden_dim = 256, dropout=0.3).to(device)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=target_word2ind['<PAD>'])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), weight_decay = 0.000001)\n",
    "# train_losses, val_losses = train_transformer(model, criterion, optimizer, dataloader, eval_dataloader, 10)\n",
    "# from utils import translate\n",
    "# from tqdm import tqdm\n",
    "# data = read_json('test_no_reference')\n",
    "# result = []\n",
    "# for line in tqdm(data):\n",
    "#     sentence = [char for char in line['src']]\n",
    "#     translated_sentence = translate(model, sentence, source_word2ind, target_word2ind)\n",
    "#     # print(translated_sentence)\n",
    "#     result.append({'src': line['src'],\n",
    "#                    'dst': ''.join(translated_sentence)})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
