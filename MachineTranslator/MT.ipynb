{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412c94f4-c7f6-46dc-82b9-104125ef8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from WordDataset import WordDataset\n",
    "from Translator import Translator\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f033578-275d-4e94-ae1b-fcbaa7100fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = []\n",
    "target = []\n",
    "\n",
    "with open('rus.txt') as f:\n",
    "    for line in f:\n",
    "        t, s = line.split('\\t')[:2]\n",
    "        target.append(s.lower())\n",
    "        source.append(t.lower())\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "target_bag_of_words = []\n",
    "source_bag_of_words = []\n",
    "\n",
    "target_sentences = []\n",
    "source_sentences = []\n",
    "\n",
    "for i in range(len(target)):\n",
    "    t_sent = target[i]\n",
    "    s_sent = source[i]\n",
    "    t_tokens = tokenizer.tokenize(t_sent)\n",
    "    s_tokens = tokenizer.tokenize(s_sent)\n",
    "    \n",
    "    target_bag_of_words.extend(t_tokens)\n",
    "    source_bag_of_words.extend(s_tokens)\n",
    "\n",
    "    target_sentences.append(t_tokens)\n",
    "    source_sentences.append(s_tokens)\n",
    "    \n",
    "\n",
    "special_symbols = ['<SOS>', '<EOS>', '<PAD>', '<UNK>']\n",
    "\n",
    "target_bag_of_words.extend(special_symbols)\n",
    "source_bag_of_words.extend(special_symbols)\n",
    "target_bag_of_words = set(target_bag_of_words)\n",
    "source_bag_of_words = set(source_bag_of_words)\n",
    "\n",
    "source_word2ind = {word: ind for ind, word in enumerate(source_bag_of_words)}\n",
    "target_word2ind = {word: ind for ind, word in enumerate(target_bag_of_words)}\n",
    "source_ind2word = {ind: word for ind, word in enumerate(source_bag_of_words)}\n",
    "target_ind2word = {ind: word for ind, word in enumerate(target_bag_of_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a7e47d-5cfa-4fce-8381-cc8ff991528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del target_bag_of_words\n",
    "del source_bag_of_words\n",
    "del special_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e359bea2-c10d-41a2-900f-6c6c9ee76008",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(max([len(sentence) for sentence in target_sentences]), max([len(sentence) for sentence in source_sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37042454-ce5a-46e4-8b23-5180eab999f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordDataset(source_sentences, target_sentences, source_word2ind, target_word2ind, max_len = 2)\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ecc545e-c992-4a24-a117-a77410291635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloader, num_epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(1, num_epoch+1):\n",
    "        print(f'epoch:{epoch}')\n",
    "        for source, target in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            target_input = target[:, :-1].to(device)\n",
    "            target_output = target[:, 1:].to(device).flatten(start_dim = 0, end_dim = 1)\n",
    "\n",
    "            outp = model(source.to(device), target_input).squeeze()\n",
    "            outp = outp.flatten(start_dim = 0, end_dim = 1)\n",
    "\n",
    "            \n",
    "            loss = criterion(outp.to(device), target_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    \n",
    "    return losses\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3bdca9-906b-449e-b348-9bb54f2e7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53270\n"
     ]
    }
   ],
   "source": [
    "print(len(target_word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82925db7-f427-4d44-8bc1-742f20078c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator(len(source_word2ind), len(target_word2ind), 400, dropout_prob=0.2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "428fc442-93dc-4db2-a12c-bde311eb2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3653/7268 [01:54<01:52, 32.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m train_model(model, criterion, optimizer, dataloader, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, dataloader, num_epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = train_model(model, criterion, optimizer, dataloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c279919-6bf7-42a3-a143-0f2727d644a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f171a6c-b999-4fb7-b26a-52fa22930d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def translate_sentence(model, sentence, tokenizer, device='cuda', max_length=5, temperature = 1):\n",
    "    model.eval()\n",
    "    \n",
    "    source_tokens = tokenizer.tokenize(sentence)\n",
    "    # Добавляем размерность батча\n",
    "    # source_tensor = torch.LongTensor([[source_word2ind['<SOS>']]+[source_word2ind[word] for word in source_tokens]+[source_word2ind['<EOS>']]]).to(device)\n",
    "    source_tensor = torch.LongTensor([[source_word2ind[word] for word in source_tokens]]).to(device)\n",
    "    target_tokens = [target_word2ind['<SOS>']]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        source_embeddings = model.source_embeddings(source_tensor)  \n",
    "        _, encoded_hidden = model.encoder(source_embeddings) \n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            target_tensor = torch.LongTensor([target_tokens]).to(device) \n",
    "            \n",
    "            target_embeddings = model.target_embeddings(target_tensor)  \n",
    "            \n",
    "            output, _ = model.decoder(target_embeddings, encoded_hidden)\n",
    "            output = model.non_lin(model.linear(model.non_lin(output)))\n",
    "            logits = model.projection(output)/temperature\n",
    "            \n",
    "            probabilities = F.softmax(logits[0, -1], dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            target_tokens.append(next_token)\n",
    "            \n",
    "            if next_token == target_word2ind['<EOS>']:\n",
    "                break\n",
    "    \n",
    "    translated_tokens = target_tokens[1:-1] \n",
    "    translated_sentence = \" \".join(target_ind2word[idx] for idx in translated_tokens)\n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e4fcd-1df4-4804-9998-745ebf5f830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence(model, 'hip',  tokenizer, max_length = 10, temperature = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240e31d-6fe2-454a-b6c8-73b057ada162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
