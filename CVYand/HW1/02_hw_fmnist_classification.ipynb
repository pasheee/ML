{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDsVMGiVgSq2"
   },
   "source": [
    "## Классификация FashionMNIST\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/girafe_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3isBRG6PgSq6"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "\n",
    "def parse_pytorch_model(model_str):\n",
    "    def parse_layer(layer_str):\n",
    "        layer_info = {}\n",
    "        layer_name, params = layer_str.split(\"(\", 1)\n",
    "        params = params.rstrip(\")\")\n",
    "        layer_info[\"type\"] = layer_name.strip()\n",
    "        param_dict = {}\n",
    "        for param in params.split(\", \"):\n",
    "            if \"=\" in param:\n",
    "                key, value = param.split(\"=\")\n",
    "                param_dict[key.strip()] = eval(value.strip())\n",
    "            else:\n",
    "                param_dict[param.strip()] = None\n",
    "        layer_info[\"parameters\"] = param_dict\n",
    "        return layer_info\n",
    "\n",
    "    model_dict = {}\n",
    "    lines = model_str.splitlines()\n",
    "    model_name = lines[0].strip(\"()\")\n",
    "    model_dict[\"model_name\"] = model_name\n",
    "    model_dict[\"layers\"] = []\n",
    "\n",
    "    layer_regex = re.compile(r\"\\((\\d+)\\): (.+)\")\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip()\n",
    "        match = layer_regex.match(line)\n",
    "        if match:\n",
    "            index, layer = match.groups()\n",
    "            model_dict[\"layers\"].append({\"index\": int(index), \"layer\": parse_layer(layer)})\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "\n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def get_accuracy(model, data_loader):\n",
    "    predicted_labels = []\n",
    "    real_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            y_predicted = model(batch[0].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "            real_labels.append(batch[1])\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    real_labels = torch.cat(real_labels)\n",
    "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
    "    return accuracy_score\n",
    "\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_fmnist_data_dict.npy\"\n",
    "), \"Please, download `hw_fmnist_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeA6Q5-CgSq7"
   },
   "source": [
    "Вернемся к задаче распознавания простых изображений, рассмотренной ранее. Но теперь будем работать с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша первая задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбукам с предыдущих занятий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE_ID = 0  # change if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nPG1KbQAgl8b"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = (\n",
    "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "aYcL28OsgSq8",
    "outputId": "93aafa07-fb56-43bd-f928-918f45fe30e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 4')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_fmnist_data = FashionMNIST(\n",
    "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_fmnist_data = FashionMNIST(\n",
    "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_fmnist_data, batch_size=1024, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_fmnist_data, batch_size=1024, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f\"Image label: {_label}\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6jWRv1rgSq8"
   },
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CV_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolution blocks:\n",
    "        self.c1 = nn.Conv2d(1, 32, 5)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.c2 = nn.Conv2d(32, 64, 3)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.c3 = nn.Conv2d(64, 128, 3)\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.conv_non_lin = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Linear Blocks:\n",
    "        self.l1 = nn.Linear(128 * 5 * 5, 512)\n",
    "        self.l2 = nn.Linear(512, 124)\n",
    "        self.projection = nn.Linear(124, 10)\n",
    "        self.non_lin = nn.Tanh()\n",
    "\n",
    "    def forward(self, X):\n",
    "        conv_out = self.conv_non_lin( self.norm2( self.pooling( self.c2( self.norm1( self.conv_non_lin(self.c1(X)) ) ) ) ) )\n",
    "        conv_out = self.conv_non_lin( self.pooling( self.c3(conv_out) ) )\n",
    "        flattened_conv_out = self.flatten(conv_out)\n",
    "\n",
    "        return self.projection( self.non_lin( self.l2( self.non_lin(self.dropout(self.l1(flattened_conv_out))) ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BcyEFX-RgSq8"
   },
   "outputs": [],
   "source": [
    "# Creating model instance\n",
    "model_task_1 = CV_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAoLV4dkoy5M"
   },
   "source": [
    "Не забудьте перенести модель на выбранный `device`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Xas9SIXDoxvZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV_Model(\n",
       "  (c1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (c3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (conv_non_lin): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (l1): Linear(in_features=3200, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=124, bias=True)\n",
       "  (projection): Linear(in_features=124, out_features=10, bias=True)\n",
       "  (non_lin): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task_1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pLRWysggSq9"
   },
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qMQzo1ggSq9",
    "outputId": "c00008eb-ef88-4000-ce47-e8dedd26e061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something is wrong with the model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x2048 and 3200x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSomething is wrong with the model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m y_predicted.shape[-\u001b[32m1\u001b[39m] == \u001b[32m10\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mModel should predict 10 logits/probas\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEverything seems fine!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     y = random_batch[\u001b[32m1\u001b[39m].to(device)\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# compute outputs given inputs, both are variables\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     y_predicted = \u001b[43mmodel_task_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSomething is wrong with the model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mCV_Model.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     26\u001b[39m conv_out = \u001b[38;5;28mself\u001b[39m.conv_non_lin( \u001b[38;5;28mself\u001b[39m.pooling( \u001b[38;5;28mself\u001b[39m.c3(conv_out) ) )\n\u001b[32m     27\u001b[39m flattened_conv_out = \u001b[38;5;28mself\u001b[39m.flatten(conv_out)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.projection( \u001b[38;5;28mself\u001b[39m.non_lin( \u001b[38;5;28mself\u001b[39m.l2( \u001b[38;5;28mself\u001b[39m.non_lin(\u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_conv_out\u001b[49m\u001b[43m)\u001b[49m)) ) ) )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1024x2048 and 3200x512)"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].to(device)\n",
    "    y = random_batch[1].to(device)\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model_task_1(x)\n",
    "except Exception as e:\n",
    "    print(\"Something is wrong with the model\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
    "\n",
    "print(\"Everything seems fine!\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suRmIPwIgSq9"
   },
   "source": [
    "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_task_1.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=10)\n",
    "num_epochs = 15\n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x2048 and 3200x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\u001b[32m      8\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m y_pred = \u001b[43mmodel_task_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m loss = criterion(y_pred, y_batch)\n\u001b[32m     12\u001b[39m train_loss_history.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mCV_Model.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     26\u001b[39m conv_out = \u001b[38;5;28mself\u001b[39m.conv_non_lin( \u001b[38;5;28mself\u001b[39m.pooling( \u001b[38;5;28mself\u001b[39m.c3(conv_out) ) )\n\u001b[32m     27\u001b[39m flattened_conv_out = \u001b[38;5;28mself\u001b[39m.flatten(conv_out)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.projection( \u001b[38;5;28mself\u001b[39m.non_lin( \u001b[38;5;28mself\u001b[39m.l2( \u001b[38;5;28mself\u001b[39m.non_lin(\u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_conv_out\u001b[49m\u001b[43m)\u001b[49m)) ) ) )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1024x2048 and 3200x512)"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    model_task_1.train()\n",
    "    train_loss_history = []\n",
    "    for x_batch, y_batch in train_data_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model_task_1(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        train_loss_history.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Mean Train Loss: {np.mean(train_loss_history)}')\n",
    "\n",
    "    test_loss_history = []\n",
    "    for x_batch, y_batch in test_data_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model_task_1(x_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        test_loss_history.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Mean Test Loss: {np.mean(test_loss_history)}')\n",
    "    \n",
    "    test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
    "    scheduler.step(test_acc_task_1)\n",
    "    print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")\n",
    "    \n",
    "    if test_acc_task_1 > best_score:\n",
    "        best_score = test_acc_task_1\n",
    "        torch.save(model_task_1, 'best_model.pth')\n",
    "        print('New model saved.')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJnU14bdnZa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1024x2048 and 3200x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n\u001b[32m      9\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m y_pred = \u001b[43mmodel_task_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = criterion(y_pred, y_batch)\n\u001b[32m     13\u001b[39m train_loss_history.append(loss.item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mCV_Model.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     26\u001b[39m conv_out = \u001b[38;5;28mself\u001b[39m.conv_non_lin( \u001b[38;5;28mself\u001b[39m.pooling( \u001b[38;5;28mself\u001b[39m.c3(conv_out) ) )\n\u001b[32m     27\u001b[39m flattened_conv_out = \u001b[38;5;28mself\u001b[39m.flatten(conv_out)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.projection( \u001b[38;5;28mself\u001b[39m.non_lin( \u001b[38;5;28mself\u001b[39m.l2( \u001b[38;5;28mself\u001b[39m.non_lin(\u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_conv_out\u001b[49m\u001b[43m)\u001b[49m)) ) ) )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\ML\\CVYand\\.cv_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1024x2048 and 3200x512)"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f'Epoch: {epoch + 1}')\n",
    "#     model_task_1.train()\n",
    "#     train_loss_history = []\n",
    "#     for x_batch, y_batch in train_data_loader:\n",
    "#         x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         y_pred = model_task_1(x_batch)\n",
    "#         loss = criterion(y_pred, y_batch)\n",
    "#         train_loss_history.append(loss.item())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Mean Train Loss: {np.mean(train_loss_history)}')\n",
    "\n",
    "#     model_task_1.eval()\n",
    "#     test_loss_history = []\n",
    "#     with torch.no_grad():\n",
    "#         for x_batch, y_batch in test_data_loader:\n",
    "#             x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "#             y_pred = model_task_1(x_batch)\n",
    "#             loss = criterion(y_pred, y_batch)\n",
    "#             test_loss_history.append(loss.item())\n",
    "#     print(f'Mean Test Loss: {np.mean(test_loss_history)}')\n",
    "    \n",
    "#     test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
    "#     scheduler.step(test_acc_task_1)\n",
    "#     print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")\n",
    "    \n",
    "#     if test_acc_task_1 > best_score:\n",
    "#         best_score = test_acc_task_1\n",
    "#         torch.save(model_task_1, 'best_model.pth')\n",
    "#         print('New model saved.')\n",
    "\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_task_1 = torch.load('best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zce7gt1gSq-"
   },
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usswrWYOgSq-"
   },
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xua3TVZHgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.94662\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9KEKXBxgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.8981\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyhmMobgSq_"
   },
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "OAIrURCEgSq_",
    "outputId": "7c983690-a92e-4693-89fb-7c86c002921a"
   },
   "outputs": [],
   "source": [
    "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
    "assert (\n",
    "    train_acc_task_1 >= 0.905\n",
    "), \"Test accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`, а файл `hw_fmnist_data_dict.npy` находится в той же директории, что и ноутбук (он доступен в репозитории)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_fmnist_task_1.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_fmnist_data_dict.npy\"\n",
    "), \"Please, download `hw_fmnist_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_fmnist_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    \"train_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "    ),\n",
    "    \"test_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "    ),\n",
    "    \"model_task_1\": parse_pytorch_model(str(model_task_1)),\n",
    "}\n",
    "\n",
    "with open(\"submission_dict_fmnist_task_1.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_fmnist_task_1.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Сдайте сгенерированный файл в соответствующую задачу в соревновании, а именно:\n",
    "    \n",
    "* `submission_dict_fmnist_task_1.json` в задачу Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtWnYAN_gSrA"
   },
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".cv_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
